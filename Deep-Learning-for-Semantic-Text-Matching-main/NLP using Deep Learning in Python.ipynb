{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP using Deep Learning in Python - Quora Duplicate Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement:\n",
    "\n",
    "Over 100 million people visit Quora every month, so it's no surprise that **many people ask similarly worded questions**. Multiple questions with the same intent **can cause seekers to spend more time finding the best answer to their question**, and **make writers feel they need to answer multiple versions of the same question**. Quora values canonical questions because they **provide a better experience to active seekers and writers**, and offer more value to both of these groups in the long term.\n",
    "\n",
    "**Reference:** https://www.kaggle.com/c/quora-question-pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_pairs = pd.read_csv(\"../../raw_data/questions.csv\")\n",
    "question_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(808702, 2)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_pairs_1 = question_pairs[['qid1', 'question1']]\n",
    "question_pairs_1.columns = ['id', 'question']\n",
    "question_pairs_2 = question_pairs[['qid2', 'question2']]\n",
    "question_pairs_2.columns = ['id', 'question']\n",
    "questions_list = pd.concat([question_pairs_1,question_pairs_2]).sort_values('id')\n",
    "questions_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the step by step guide to invest in share market in india?',\n",
       " 'What is the step by step guide to invest in share market?',\n",
       " 'What is the story of Kohinoor (Koh-i-Noor) Diamond?',\n",
       " 'What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?',\n",
       " 'How can I increase the speed of my internet connection while using a VPN?',\n",
       " 'How can Internet speed be increased by hacking through DNS?',\n",
       " 'Why am I mentally very lonely? How can I solve it?',\n",
       " 'Find the remainder when [math]23^{24}[/math] is divided by 24,23?',\n",
       " 'Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?',\n",
       " 'Which fish would survive in salt water?']"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = questions_list['question'].tolist()\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(np.unique(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1000</th>\n",
       "      <th>2000</th>\n",
       "      <th>500</th>\n",
       "      <th>about</th>\n",
       "      <th>add</th>\n",
       "      <th>all</th>\n",
       "      <th>am</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>are</th>\n",
       "      <th>...</th>\n",
       "      <th>was</th>\n",
       "      <th>we</th>\n",
       "      <th>what</th>\n",
       "      <th>who</th>\n",
       "      <th>will</th>\n",
       "      <th>win</th>\n",
       "      <th>with</th>\n",
       "      <th>world</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1000  2000  500  about  add  all  am  and  any  are  ...  was  we  what  \\\n",
       "0     0     0    0      0    0    0   0    0    1    0  ...    1   0     1   \n",
       "1     1     0    1      0    0    0   0    2    0    0  ...    0   0     1   \n",
       "2     0     0    0      0    1    0   0    0    0    0  ...    0   0     0   \n",
       "3     0     0    0      0    0    1   0    0    0    0  ...    0   0     0   \n",
       "4     1     0    0      0    0    0   1    0    0    0  ...    0   0     0   \n",
       "5     0     1    1      0    0    0   0    1    0    1  ...    0   0     0   \n",
       "6     0     1    0      0    0    0   0    0    0    1  ...    0   0     0   \n",
       "7     0     0    0      1    0    0   0    0    0    1  ...    0   0     0   \n",
       "8     0     0    0      0    0    0   0    0    0    1  ...    0   1     0   \n",
       "9     0     0    0      0    0    0   0    0    0    1  ...    0   1     0   \n",
       "\n",
       "   who  will  win  with  world  you  your  \n",
       "0    0     0    0     1      0    0     0  \n",
       "1    0     1    0     0      0    0     0  \n",
       "2    0     0    0     0      0    0     0  \n",
       "3    1     1    1     0      0    1     0  \n",
       "4    0     0    0     0      0    0     0  \n",
       "5    0     0    0     0      0    0     0  \n",
       "6    0     0    0     1      0    0     0  \n",
       "7    1     0    0     1      0    1     1  \n",
       "8    0     0    0     0      1    0     0  \n",
       "9    0     0    0     0      1    0     0  \n",
       "\n",
       "[10 rows x 96 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(corpus[:10])\n",
    "X_train_counts = pd.DataFrame(X_train_counts.toarray())\n",
    "X_train_counts.columns = count_vect.get_feature_names()\n",
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"The question was marked as needing improvement\" how to deal with this, what ever I do still this error pops up? Is it Quora bot or any user?'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000             0\n",
       "2000             0\n",
       "500              0\n",
       "about            0\n",
       "add              0\n",
       "all              0\n",
       "am               0\n",
       "and              0\n",
       "any              1\n",
       "are              0\n",
       "as               1\n",
       "aside            0\n",
       "at               0\n",
       "banned           0\n",
       "based            0\n",
       "be               0\n",
       "been             0\n",
       "biases           0\n",
       "big              0\n",
       "bot              1\n",
       "can              0\n",
       "chip             0\n",
       "closer           0\n",
       "currency         0\n",
       "deal             1\n",
       "distance         0\n",
       "do               1\n",
       "election         0\n",
       "embedded         0\n",
       "error            1\n",
       "                ..\n",
       "really           0\n",
       "relationship     0\n",
       "relationships    0\n",
       "rs               0\n",
       "see              0\n",
       "short            0\n",
       "starting         0\n",
       "still            1\n",
       "successful       0\n",
       "tell             0\n",
       "term             0\n",
       "the              1\n",
       "there            0\n",
       "think            0\n",
       "this             2\n",
       "time             0\n",
       "to               1\n",
       "up               1\n",
       "user             1\n",
       "war              0\n",
       "was              1\n",
       "we               0\n",
       "what             1\n",
       "who              0\n",
       "will             0\n",
       "win              0\n",
       "with             1\n",
       "world            0\n",
       "you              0\n",
       "your             0\n",
       "Name: 0, Length: 96, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1000</th>\n",
       "      <th>2000</th>\n",
       "      <th>500</th>\n",
       "      <th>about</th>\n",
       "      <th>add</th>\n",
       "      <th>all</th>\n",
       "      <th>am</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>are</th>\n",
       "      <th>...</th>\n",
       "      <th>was</th>\n",
       "      <th>we</th>\n",
       "      <th>what</th>\n",
       "      <th>who</th>\n",
       "      <th>will</th>\n",
       "      <th>win</th>\n",
       "      <th>with</th>\n",
       "      <th>world</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.20204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.20204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.205776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.411553</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.518291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223595</td>\n",
       "      <td>0.223595</td>\n",
       "      <td>0.263025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223595</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.266593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251549</td>\n",
       "      <td>0.251549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251549</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.175717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.218880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.121303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173653</td>\n",
       "      <td>0.204276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.263628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.377400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.235430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.337033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1000      2000       500     about       add       all        am  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.205776  0.000000  0.205776  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.518291  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.263025  0.000000   \n",
       "4  0.266593  0.000000  0.000000  0.000000  0.000000  0.000000  0.313605   \n",
       "5  0.000000  0.251549  0.251549  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.313340  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.000000  0.000000  0.204276  0.000000  0.000000  0.000000   \n",
       "8  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        and      any       are  ...      was        we      what       who  \\\n",
       "0  0.000000  0.20204  0.000000  ...  0.20204  0.000000  0.171753  0.000000   \n",
       "1  0.411553  0.00000  0.000000  ...  0.00000  0.000000  0.205776  0.000000   \n",
       "2  0.000000  0.00000  0.000000  ...  0.00000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.00000  0.000000  ...  0.00000  0.000000  0.000000  0.223595   \n",
       "4  0.000000  0.00000  0.000000  ...  0.00000  0.000000  0.000000  0.000000   \n",
       "5  0.251549  0.00000  0.175717  ...  0.00000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.00000  0.218880  ...  0.00000  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.00000  0.121303  ...  0.00000  0.000000  0.000000  0.173653   \n",
       "8  0.000000  0.00000  0.263628  ...  0.00000  0.377400  0.000000  0.000000   \n",
       "9  0.000000  0.00000  0.235430  ...  0.00000  0.337033  0.000000  0.000000   \n",
       "\n",
       "       will       win      with     world       you      your  \n",
       "0  0.000000  0.000000  0.150263  0.000000  0.000000  0.000000  \n",
       "1  0.205776  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.223595  0.263025  0.000000  0.000000  0.223595  0.000000  \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "5  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "6  0.000000  0.000000  0.274136  0.000000  0.000000  0.000000  \n",
       "7  0.000000  0.000000  0.151926  0.000000  0.173653  0.204276  \n",
       "8  0.000000  0.000000  0.000000  0.377400  0.000000  0.000000  \n",
       "9  0.000000  0.000000  0.000000  0.337033  0.000000  0.000000  \n",
       "\n",
       "[10 rows x 96 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(corpus[:10])\n",
    "X_train_tfidf = pd.DataFrame(X_train_tfidf.toarray())\n",
    "X_train_tfidf.columns = vectorizer.get_feature_names()\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Tf(w, d) = Number \\ of \\ times \\ word \\ `w` \\ appears \\ in \\ document \\ `d`$$\n",
    "\n",
    "$$IDF(w) = \\log \\frac{Total \\ number \\ of \\ documents}{Number \\ of \\ documents \\ with \\ word \\ `w`}$$\n",
    "\n",
    "$$Tfidf(w, d) = Tf(w, d) * IDF(w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors\n",
    "\n",
    "Word vectors - also called *word embeddings* - are mathematical descriptions of individual words such that words that appear frequently together in the language will have similar values. In this way we can mathematically derive *context*.\n",
    "\n",
    "**There are two possible approaches:**\n",
    "\n",
    "<img src=\"img/w2v_image.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "**CBOW (Continuous Bag Of Words):** It predicts the word, given context around the word as input\n",
    "\n",
    "**Skip-gram:** It predicts the context, given the word as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp('dog').vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, topn=5):\n",
    "    word = nlp.vocab[str(word)]\n",
    "    queries = [\n",
    "      w for w in word.vocab \n",
    "      if w.is_lower == word.is_lower and w.prob >= -15 and np.count_nonzero(w.vector)\n",
    "    ]\n",
    "\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princes', 0.7876614),\n",
       " ('kings', 0.7876614),\n",
       " ('prince', 0.73377377),\n",
       " ('queen', 0.72526103),\n",
       " ('scepter', 0.6726005),\n",
       " ('throne', 0.6726005),\n",
       " ('kingdoms', 0.6604046),\n",
       " ('kingdom', 0.6604046),\n",
       " ('lord', 0.6439695),\n",
       " ('royal', 0.6168811)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"king\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cheetah', 0.9999999),\n",
       " ('lions', 0.7758893),\n",
       " ('tiger', 0.7359829),\n",
       " ('panther', 0.7359829),\n",
       " ('leopard', 0.7359829),\n",
       " ('elephant', 0.71239567),\n",
       " ('hippo', 0.71239567),\n",
       " ('zebra', 0.71239567),\n",
       " ('rhino', 0.71239567),\n",
       " ('giraffe', 0.71239567)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(\"lion\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence (or document) objects  have vectors, derived from the averages of individual token vectors. This makes it possible to compare similarities between whole documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('The quick brown fox jumped over the lazy dogs.')\n",
    "len(doc.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "embedder = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 3.71 s, total: 1min 58s\n",
      "Wall time: 34.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus_embeddings = embedder.encode(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Candidate Genration using Faiss vector similarity search library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faiss is a library developed by Facebook AI Research. It is for effecient similarity search and clustering of dense vectors.\n",
    "\n",
    "**References:**\n",
    "\n",
    "1. [Tutorial](https://github.com/facebookresearch/faiss/wiki/Getting-started)\n",
    "2. [facebookresearch/faiss](https://github.com/facebookresearch/faiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1362\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "d= 768\n",
    "index = faiss.IndexFlatL2(d)\n",
    "print(index.is_trained)\n",
    "index.add(np.stack(corpus_embeddings, axis=0))\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = ['What is the step by step guide to invest in share market in india?', 'How can Internet speed be increased by hacking through DNS?']\n",
    "queries = question_pairs['question1'][:3].tolist()\n",
    "query_embeddings = embedder.encode(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 858  998 1025  983   73]\n",
      " [ 771 1015  775 1014 1133]\n",
      " [ 436  455  457  463  462]]\n"
     ]
    }
   ],
   "source": [
    "k = 5                          # we want to see 4 nearest neighbors\n",
    "D, I = index.search(np.stack(query_embeddings, axis=0), k)     # actual search\n",
    "print(I)                   # neighbors of the 5 first queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "\n",
      "Query: What is purpose of life?\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "What is purpose of life? (Distance: 0.0000)\n",
      "What is the purpose of life? (Distance: 7.9868)\n",
      "What is your purpose of life? (Distance: 12.3884)\n",
      "What is the meaning or purpose of life? (Distance: 13.6231)\n",
      "From your perspective, what is the purpose of life? (Distance: 17.5448)\n",
      "\n",
      "======================\n",
      "\n",
      "Query: What are your New Year's resolutions for 2017?\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "What are your New Year's resolutions for 2017? (Distance: 0.0000)\n",
      "What is your New Year's resolutions for 2017? (Distance: 0.6093)\n",
      "What are your new year resolutions for 2017? (Distance: 5.8446)\n",
      "What is your New Year's resolution for 2017? (Distance: 6.6011)\n",
      "What's your New Year's resolution for 2017? (Distance: 8.1350)\n",
      "\n",
      "======================\n",
      "\n",
      "Query: How will Indian GDP be affected from banning 500 and 1000 rupees notes?\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "How will Indian GDP be affected from banning 500 and 1000 rupees notes? (Distance: 0.0000)\n",
      "How will the ban of 1000 and 500 rupee notes affect the Indian economy? (Distance: 24.4171)\n",
      "How will the ban of Rs 500 and Rs 1000 notes affect Indian economy? (Distance: 26.4862)\n",
      "How will the ban on Rs 500 and 1000 notes impact the Indian economy? (Distance: 26.5522)\n",
      "How will the ban on 500₹ and 1000₹ notes impact the Indian economy? (Distance: 28.7938)\n"
     ]
    }
   ],
   "source": [
    "for query, query_embedding in zip(queries, query_embeddings):\n",
    "    distances, indices = index.search(np.asarray(query_embedding).reshape(1,768),k)\n",
    "    print(\"\\n======================\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "    for idx in range(0,5):\n",
    "        print(corpus[indices[0,idx]], \"(Distance: %.4f)\" % distances[0,idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How will Indian GDP be affected from banning 500 and 1000 rupees notes?\"\n",
    "query_embed = embedder.encode(query)\n",
    "distances, indices = index.search(np.asarray(query_embedding).reshape(1,768),50)\n",
    "relevant_docs = [corpus[indices[0,idx]] for idx in range(50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Reranking using Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/bi_lstm.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Reference:** https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "toko_tokenizer = ToktokTokenizer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize_text(text):\n",
    "        puncts = ['/', ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    "         '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    "         '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    "         '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    "         '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "        def clean_text(text):\n",
    "            text = str(text)\n",
    "            text = text.replace('\\n', '')\n",
    "            text = text.replace('\\r', '')\n",
    "            for punct in puncts:\n",
    "                if punct in text:\n",
    "                    text = text.replace(punct, '')\n",
    "            return text.lower()\n",
    "\n",
    "        def clean_numbers(text):\n",
    "            if bool(re.search(r'\\d', text)):\n",
    "                text = re.sub('[0-9]{5,}', '#####', text)\n",
    "                text = re.sub('[0-9]{4}', '####', text)\n",
    "                text = re.sub('[0-9]{3}', '###', text)\n",
    "                text = re.sub('[0-9]{2}', '##', text)\n",
    "            return text\n",
    "\n",
    "        contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "        def _get_contractions(contraction_dict):\n",
    "            contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "            return contraction_dict, contraction_re\n",
    "\n",
    "        contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "        def replace_contractions(text):\n",
    "            def replace(match):\n",
    "                return contractions[match.group(0)]\n",
    "            return contractions_re.sub(replace, text)\n",
    "\n",
    "        stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "        def remove_stopwords(text, is_lower_case=True):\n",
    "            tokens = toko_tokenizer.tokenize(text)\n",
    "            tokens = [token.strip() for token in tokens]\n",
    "            if is_lower_case:\n",
    "                filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "            else:\n",
    "                filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "            filtered_text = ' '.join(filtered_tokens)    \n",
    "            return filtered_text\n",
    "\n",
    "        def lemmatizer(text):\n",
    "            tokens = toko_tokenizer.tokenize(text)\n",
    "            tokens = [token.strip() for token in tokens]\n",
    "            tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "            return ' '.join(tokens)\n",
    "\n",
    "        def trim_text(text):\n",
    "            tokens = toko_tokenizer.tokenize(text)\n",
    "            tokens = [token.strip() for token in tokens]\n",
    "            return ' '.join(tokens)\n",
    "        \n",
    "        def remove_non_english(text):\n",
    "            tokens = toko_tokenizer.tokenize(text)\n",
    "            tokens = [token.strip() for token in tokens]\n",
    "            tokens = [token for token in tokens if d.check(token)]\n",
    "            eng_text = ' '.join(tokens)\n",
    "            return eng_text\n",
    "\n",
    "        text_norm = clean_text(text)\n",
    "        text_norm = clean_numbers(text_norm)\n",
    "        text_norm = replace_contractions(text_norm)\n",
    "#         text_norm = remove_stopwords(text_norm)\n",
    "#         text_norm = remove_non_english(text_norm)\n",
    "        text_norm = lemmatizer(text_norm)\n",
    "        text_norm = trim_text(text_norm)\n",
    "        return text_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404351, 6)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = \"./../../Embeddings/glove.twitter.27B/glove.twitter.27B.200d.txt\"\n",
    "def get_word2vec(file_path):\n",
    "    file = open(embedding_path, \"r\")\n",
    "    if (file):\n",
    "        word2vec = dict()\n",
    "        split = file.read().splitlines()\n",
    "        for line in split:\n",
    "            key = line.split(' ',1)[0]\n",
    "            value = np.array([float(val) for val in line.split(' ')[1:]])\n",
    "            word2vec[key] = value\n",
    "        return (word2vec)\n",
    "    else:\n",
    "        print(\"invalid fiel path\")\n",
    "w2v = get_word2vec(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text = pd.concat([question_pairs['question1'], question_pairs['question2']]).reset_index(drop=True)\n",
    "total_text = total_text.apply(lambda x: str(x))\n",
    "total_text = total_text.apply(lambda x: normalize_text(x))\n",
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(total_text)\n",
    "question_1_sequenced = tokenizer.texts_to_sequences(question_pairs['question1'].apply(lambda x: normalize_text(x)))\n",
    "question_2_sequenced = tokenizer.texts_to_sequences(question_pairs['question2'].apply(lambda x: normalize_text(x)))\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92423"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "question_1_padded = pad_sequences(question_1_sequenced, maxlen=maxlen)\n",
    "question_2_padded = pad_sequences(question_2_sequenced, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = question_pairs['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "embedding_matrix = zeros((vocab_size, 768))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = w2v.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "max_len = 100\n",
    "\n",
    "inp1 = Input(shape=(100,))\n",
    "inp2 = Input(shape=(100,))\n",
    "\n",
    "x1 = Embedding(vocab_size, 200, weights=[embedding_matrix], input_length=max_len)(inp1)\n",
    "x2 = Embedding(vocab_size, 200, weights=[embedding_matrix], input_length=max_len)(inp2)\n",
    "\n",
    "x3 = Bidirectional(LSTM(32, return_sequences = True))(x1)\n",
    "x4 = Bidirectional(LSTM(32, return_sequences = True))(x2)\n",
    "\n",
    "x5 = GlobalMaxPool1D()(x3)\n",
    "x6 = GlobalMaxPool1D()(x4)\n",
    "\n",
    "x7 =  dot([x5, x6], axes=1)\n",
    "\n",
    "x8 = Dense(40, activation='relu')(x7)\n",
    "x9 = Dropout(0.05)(x8)\n",
    "x10 = Dense(10, activation='relu')(x9)\n",
    "output = Dense(1, activation=\"sigmoid\")(x10)\n",
    "\n",
    "model = Model(inputs=[inp1, inp2], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "batch_size = 256\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323480 samples, validate on 80871 samples\n",
      "Epoch 1/4\n",
      "323480/323480 [==============================] - 1096s 3ms/step - loss: 0.5212 - acc: 0.7372 - val_loss: 0.4596 - val_acc: 0.7802\n",
      "Epoch 2/4\n",
      "323480/323480 [==============================] - 1101s 3ms/step - loss: 0.4169 - acc: 0.8038 - val_loss: 0.4300 - val_acc: 0.7978\n",
      "Epoch 3/4\n",
      "323480/323480 [==============================] - 1135s 4ms/step - loss: 0.3536 - acc: 0.8400 - val_loss: 0.4340 - val_acc: 0.7976\n",
      "Epoch 4/4\n",
      "323480/323480 [==============================] - 1077s 3ms/step - loss: 0.2963 - acc: 0.8701 - val_loss: 0.4411 - val_acc: 0.8035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bd352da0>"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([question_1_padded, question_2_padded], y, batch_size=batch_size, epochs=epochs, validation_split=0.2, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## Combining candidate generation and reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How will Indian GDP be affected from banning 500 and 1000 rupees notes?'"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_copy = [query]*len(relevant_docs)\n",
    "question_1_sequenced_final = tokenizer.texts_to_sequences(query_copy)\n",
    "question_2_sequenced_final = tokenizer.texts_to_sequences(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "question_1_padded_final = pad_sequences(question_1_sequenced_final, maxlen=maxlen)\n",
    "question_2_padded_final = pad_sequences(question_2_sequenced_final, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = model.predict([question_1_padded_final, question_2_padded_final])\n",
    "preds_test = np.array([x[0] for x in preds_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What do you think about banning 500 and 1000 rupee notes in India?',\n",
       " 'What will be the implications of banning 500 and 1000 rupees currency notes on Indian economy?',\n",
       " 'What will be the consequences of 500 and 1000 rupee notes banning?',\n",
       " 'What will be the effects after banning on 500 and 1000 rupee notes?',\n",
       " 'What will be the impact on real estate by banning 500 and 1000 rupee notes from India?',\n",
       " 'How is banning 500 and 1000 INR going to help Indian economy?',\n",
       " 'What are your views on India banning 500 and 1000 notes? In what way it will affect Indian economy?',\n",
       " 'How is discontinuing 500 and 1000 rupee note going to put a hold on black money in India?',\n",
       " 'What will be the result of banning 500 and 1000 rupees note in India?',\n",
       " 'What are the economic implications of banning 500 and 1000 rupee notes?']"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[relevant_docs[x] for x in preds_test.argsort()[::-1]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
